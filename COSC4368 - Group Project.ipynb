{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e766d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a4ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment module\n",
    "class Environment:\n",
    "    def __init__(self, pick_up, drop_off):\n",
    "        # Generate initital random location for 'Female' agent\n",
    "        x, y, z = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "        \n",
    "        # Generate initial random location for 'Male' agent\n",
    "        x_p, y_p, z_p = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "        # make sure the location is available\n",
    "        while x_p == x and y_p == y and z_p == z:\n",
    "            x_p, y_p, z_p = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "            \n",
    "        # Define the shape of the environment (i.e., its states)\n",
    "        self.states = (x, y, z, x_p, y_p, z_p, 0, 0, 0, 0, 0, 0, 10, 10)\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        self.pick_up = pick_up\n",
    "        self.drop_off = drop_off\n",
    "    \n",
    "    def reset(self, pick_up, drop_off):\n",
    "        self.__init__(pick_up, drop_off)\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(self.states)\n",
    "        return female_agent_state, male_agent_state\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "    # Determine if a state is a terminal state\n",
    "        return state[6] == 0 and state[7] == 0 and state[8] == 5 and state[9] == 5 and state[10] == 5 \\\n",
    "            and state[11] == 5 and state[12] == 0 and state[13] == 0\n",
    "    \n",
    "    def update(self, agent_state, action, isFemale):\n",
    "        # Extract variables\n",
    "        x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f = self.states\n",
    "        \n",
    "        # Update the environment after the agent chose an action\n",
    "        xx, yy, zz, ii = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        pickup_list = [e, f]\n",
    "        dropoff_list = [a, b, c, d]\n",
    "        \n",
    "        # Decrease number of pick up at a specific location\n",
    "        if ActionSpace().actions[action] == 'pickup':\n",
    "            for j in range(len(pickup_list)):\n",
    "                if self.pick_up[j][0] == (xx, yy, zz):\n",
    "                    self.pick_up[j][1] -= 1\n",
    "                    pickup_list[j] = self.pick_up[j][1]\n",
    "                    break\n",
    "                \n",
    "        # Decrease number of drop off at a specific location\n",
    "        elif ActionSpace().actions[action] == 'dropoff':\n",
    "            for j in range(len(dropoff_list)):\n",
    "                if self.drop_off[j][0] == (xx, yy, zz):\n",
    "                    self.drop_off[j][1] += 1\n",
    "                    dropoff_list[j] = self.drop_off[j][1]\n",
    "                    break\n",
    "        \n",
    "        e, f = pickup_list[0], pickup_list[1]\n",
    "        a, b, c, d = dropoff_list[0], dropoff_list[1], dropoff_list[2], dropoff_list[3]\n",
    "        \n",
    "        # if female agent takes action\n",
    "        if isFemale:\n",
    "            x, y, z, i = xx, yy, zz, ii\n",
    "        # if male agent takes action\n",
    "        else:\n",
    "            x_p, y_p, z_p, i_p = xx, yy, zz, ii\n",
    "            \n",
    "        # update new state\n",
    "        self.states = (x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f)\n",
    "    \n",
    "        # check if new state is terminal state\n",
    "        done = self.is_terminal_state(self.states)\n",
    "        \n",
    "        # convert from world state to agents' state\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(self.states)\n",
    "        \n",
    "        return done, female_agent_state, male_agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d280e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent module\n",
    "class Agent:\n",
    "    def __init__(self, state, algorithm, policy, alpha, gamma):\n",
    "        self.q_table = ValueFunction(alpha, gamma)\n",
    "        self.algorithm = algorithm\n",
    "        self.state = state\n",
    "        self.policy = policy\n",
    "        self.reward = 0\n",
    "        self.pickup_state_count = 0\n",
    "        self.dropoff_state_count = 0\n",
    "        self.risky_state_count = 0\n",
    "        \n",
    "    def choose_action(self, environment, agent_state, other_agent_state):\n",
    "        possible_actions = ActionSpace().get_possible_actions(environment, agent_state, other_agent_state)\n",
    "        action = self.policy(self.q_table, agent_state, possible_actions)\n",
    "        return action\n",
    "    \n",
    "    def take_action(self, environment, action, other_agent_state):\n",
    "    \n",
    "\n",
    "#         print(\"Agent picking action: \", ActionSpace().actions[action])\n",
    "        \n",
    "        # Determine next state and reward if the agent take the action from its current state\n",
    "        new_state, reward = Model().predict(self, environment.states, action)\n",
    "        \n",
    "        # Q-Learning algorithm\n",
    "        if self.algorithm == \"QLEARNING\":\n",
    "            self.q_table.QLEARNING(self.state, action, new_state, reward)\n",
    "            \n",
    "        # SARSA algorithm\n",
    "        elif self.algorithm == \"SARSA\":\n",
    "            new_action = self.choose_action(environment, new_state, other_agent_state)\n",
    "            self.q_table.SARSA(self.state, action, new_state, new_action, reward)\n",
    "        \n",
    "        # move to new state\n",
    "        self.state = new_state\n",
    "        \n",
    "        # add reward\n",
    "        self.reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "42d699e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State representation module\n",
    "class StateRepresentation:\n",
    "    def preprocess(self, state):\n",
    "        # Convert the state into a format that can be used by the agent\n",
    "        (x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f) = state\n",
    "        \n",
    "        # Convert into female agent state\n",
    "        if i == 0:\n",
    "            s, t, u, v = state[12] >= 1, state[13] >= 1, False, False\n",
    "        else:\n",
    "            s, t, u, v = state[8] < 5, state[9] < 5, state[10] < 5, state[11] < 5\n",
    "         \n",
    "        female_agent_state = (x, y, z, i, s, t, u, v)\n",
    "        \n",
    "        # Convert into male agent state\n",
    "        if i_p == 0:\n",
    "            s, t, u, v = state[12] >= 1, state[13] >= 1, False, False\n",
    "        else:\n",
    "            s, t, u, v = state[8] < 5, state[9] < 5, state[10] < 5, state[11] < 5\n",
    "         \n",
    "        male_agent_state = (x_p, y_p, z_p, i_p, s, t, u, v)\n",
    "            \n",
    "        return female_agent_state, male_agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "611ec56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action space module\n",
    "class ActionSpace:\n",
    "    def __init__(self):\n",
    "        # Initialize the action space with the possible actions\n",
    "        self.actions = ['north', 'east', 'south', 'west', 'up', 'down', 'pickup', 'dropoff']\n",
    "    \n",
    "    # Check if action 'pick up' is applicable given an agent state\n",
    "    def CheckPickUp(self, pick_up, agent_state):\n",
    "        # Extract variables\n",
    "        x, y, z, i = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        # check if pick up is applicable\n",
    "        for locations in pick_up:\n",
    "            if locations[0] == (x, y, z) and locations[1] > 0 and i == 0:\n",
    "                return True\n",
    "    \n",
    "    # Check if action 'drop off' is applicable given an agent state\n",
    "    def CheckDropOff(self, drop_off, agent_state):\n",
    "        # Extract variables\n",
    "        x, y, z, i = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        # check if drop off is applicable\n",
    "        for locations in drop_off:\n",
    "            if locations[0] == (x, y, z) and locations[1] < 5 and i == 1:\n",
    "                return True\n",
    "    \n",
    "    # Return a list of possible actions given the agent state and other agent state (this also handle collision between agents)\n",
    "    def get_possible_actions(self, environment, agent_state, other_agent_state):\n",
    "        \n",
    "        # Initiate all actions '0'\n",
    "        possible_actions = np.zeros(8)\n",
    "        \n",
    "        # Extract agent's location and other agent's location\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "    \n",
    "        # Check if agent can move north\n",
    "        if agent_state[1] < 3 and (x, y+1, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[0] = 1\n",
    "        # Check if agent can move east\n",
    "        if agent_state[0] < 3 and (x+1, y, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[1] = 1\n",
    "        # Check if agent can move south\n",
    "        if agent_state[1] > 1 and (x, y-1, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[2] = 1    \n",
    "        # Check if agent can move west\n",
    "        if agent_state[0] > 1 and (x-1, y, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[3] = 1\n",
    "        # Check if agent can move up\n",
    "        if agent_state[2] < 3 and (x, y, z+1) != (x_p, y_p, z_p):\n",
    "            possible_actions[4] = 1\n",
    "        # Check if agent can move down\n",
    "        if agent_state[2] > 1 and (x, y, z-1) != (x_p, y_p, z_p):\n",
    "            possible_actions[5] = 1\n",
    "        # Check if agent can pick up\n",
    "        if self.CheckPickUp(environment.pick_up, agent_state):\n",
    "            possible_actions[6] = 1\n",
    "        # Check if agent can drop off\n",
    "        if self.CheckDropOff(environment.drop_off, agent_state):\n",
    "            possible_actions[7] = 1\n",
    "        \n",
    "        return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a6f792a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function module\n",
    "class RewardFunction:\n",
    "    def __init__(self):\n",
    "        # Define rewards for each state\n",
    "        self.rewards = np.full((4, 4, 4), -1)\n",
    "        self.rewards[3, 1, 1] = -2 # Risky state\n",
    "        self.rewards[2, 2, 2] = -2 # Risky state\n",
    "\n",
    "    # Calculate the reward based on an action that takes agent to the given state\n",
    "    def calculate(self, state, action):\n",
    "        \n",
    "        # Agent's current location\n",
    "        x, y, z = state[0], state[1], state[2]\n",
    "\n",
    "        # Reward\n",
    "        reward = -1\n",
    "        \n",
    "        # If action was 'pick up' or 'drop off'\n",
    "        if action == 6 or action == 7:\n",
    "            reward = 14\n",
    "        # If action was moving agent\n",
    "        else:\n",
    "            reward = self.rewards[x, y, z]\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "da6e4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy module\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.Actions = ActionSpace()\n",
    "    \n",
    "    def PRANDOM(self, q_table, agent_state, possible_actions):   \n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # choosing action randomly\n",
    "        action = np.random.randint(6)\n",
    "        while possible_actions[action] == 0:\n",
    "            action = np.random.randint(6)\n",
    "            \n",
    "        return action   \n",
    "    \n",
    "    def PEXPLOIT(self, q_table, agent_state, possible_actions):        \n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # 85% choosing the optimal action (action with highest q-value)\n",
    "        if np.random.random() < 0.85:\n",
    "            maxq = -100\n",
    "\n",
    "            for actions in range(6):\n",
    "                if possible_actions[actions] == 1:\n",
    "                    if q_table.get_value(agent_state, actions) > maxq:\n",
    "                        maxq = q_table.get_value(agent_state, actions)\n",
    "                        action = actions\n",
    "                    elif q_table.get_value(agent_state, actions) == maxq:\n",
    "                        action = action if np.random.randint(2) == 0 else actions\n",
    "        else:\n",
    "            # 15% choosing action randomly\n",
    "            action = np.random.randint(6)\n",
    "            while possible_actions[action] == 0:\n",
    "                action = np.random.randint(6)\n",
    "            \n",
    "        return action   \n",
    "    \n",
    "    \n",
    "    def PGREEDY(self, q_table, agent_state, possible_actions):\n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # choosing action with the highest q-value\n",
    "        maxq = -100\n",
    "        \n",
    "        for actions in range(6):\n",
    "            if possible_actions[actions] == 1:\n",
    "                if q_table.get_value(agent_state, actions) > maxq:\n",
    "                    maxq = q_table.get_value(agent_state, actions)\n",
    "                    action = actions\n",
    "                elif q_table.get_value(agent_state, actions) == maxq:\n",
    "                    action = action if np.random.randint(2) == 0 else actions\n",
    "                \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f6d2417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function module\n",
    "class ValueFunction:\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {}\n",
    "        \n",
    "        # Initiate q-table (dictionary with key is state, and value is list of 6 actions)\n",
    "        # q_table[state][action] gives the actual q-value of a state-action pair\n",
    "        for x in range(1, 4):\n",
    "            for y in range(1, 4):\n",
    "                for z in range(1, 4):\n",
    "                    for i in range(0, 2):\n",
    "                        for s in range(0, 2):\n",
    "                            for t in range(0, 2):\n",
    "                                for u in range(0, 2):\n",
    "                                    for v in range(0, 2):\n",
    "                                        self.q_table[(x, y, z, i, s, t, u, v)] = []\n",
    "                                        for action in range(8):\n",
    "                                            self.q_table[(x, y, z, i, s, t, u, v)].append(0)\n",
    "                                            \n",
    "    # Update Q-Table\n",
    "    def QLEARNING(self, state, action, new_state, reward):\n",
    "        self.q_table[state][action] = self.q_table[state][action] \\\n",
    "                                            + self.alpha*(reward + self.gamma*np.max(self.q_table[new_state]) \\\n",
    "                                            - self.q_table[state][action])\n",
    "        \n",
    "    def SARSA(self, state, action, new_state, new_action, reward):\n",
    "        self.q_table[state][action] = self.q_table[state][action] \\\n",
    "                                            + self.alpha*(reward + self.gamma*self.q_table[new_state][new_action] \\\n",
    "                                            - self.q_table[state][action])\n",
    "        \n",
    "    # Get a specific Q-value in the Q-Table\n",
    "    def get_value(self, state, action):\n",
    "        return self.q_table[state][action]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b3f2a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model module\n",
    "class Model:\n",
    "    \n",
    "    # Return next state and its reward given the agent current state, world state, and action performed by the agent\n",
    "    def predict(self, agent, states, action):\n",
    "        \n",
    "        # Extract variables\n",
    "        agent_state = agent.state\n",
    "        x, y, z, i, s, t, u, v = agent_state[0], agent_state[1], agent_state[2], agent_state[3], agent_state[4], agent_state[5], agent_state[6], agent_state[7]\n",
    "        \n",
    "        # move north\n",
    "        if ActionSpace().actions[action] == 'north' and y < 3:\n",
    "            y = y + 1\n",
    "        # move east\n",
    "        elif ActionSpace().actions[action] == 'east' and x < 3:\n",
    "            x = x + 1\n",
    "        # move south\n",
    "        elif ActionSpace().actions[action] == 'south' and y > 1:\n",
    "            y = y - 1\n",
    "        # move west\n",
    "        elif ActionSpace().actions[action] == 'west' and x > 1:\n",
    "            x = x - 1\n",
    "        # move up\n",
    "        elif ActionSpace().actions[action] == 'up' and z < 3:\n",
    "            z = z + 1\n",
    "        # move down\n",
    "        elif ActionSpace().actions[action] == 'down' and z > 1:\n",
    "            z = z - 1\n",
    "        # pick up\n",
    "        elif ActionSpace().actions[action] == 'pickup' and i == 0:\n",
    "            i = 1\n",
    "            agent.pickup_state_count += 1\n",
    "        # drop off\n",
    "        elif ActionSpace().actions[action] == 'dropoff' and i == 1:\n",
    "            i = 0\n",
    "            agent.dropoff_state_count += 1\n",
    "        if i == 0:\n",
    "            s = 1 if states[12] >= 1 else 0\n",
    "            t = 1 if states[13] >= 1 else 0\n",
    "            u = 0\n",
    "            v = 0\n",
    "            \n",
    "        else:\n",
    "            s = 1 if states[8] < 5 else 0\n",
    "            t = 1 if states[9] < 5 else 0\n",
    "            u = 1 if states[10] < 5 else 0\n",
    "            v = 1 if states[11] < 5 else 0\n",
    "        \n",
    "        # new state\n",
    "        next_state = (x, y, z, i, s, t, u, v)\n",
    "        \n",
    "        # reward\n",
    "        reward = RewardFunction().calculate(next_state, action)\n",
    "        \n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "dbe3cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm module\n",
    "class Algorithm:  \n",
    "    def __init__(self):\n",
    "        #Risky locations\n",
    "        self.risky_locations = [(2, 2, 2), (3, 2, 1)]\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        self.pickup_locations = [[(2, 2, 1), 10], [(3, 3, 2), 10]]\n",
    "        self.dropoff_locations = [[(1, 1, 2), 0], [(1, 1, 3), 0], [(3, 1, 1), 0], [(3, 2, 3), 0]]\n",
    "        \n",
    "    # Compute Manhattan distance between 2 agents given their states\n",
    "    def compute_distance(self, agent_state, other_agent_state):\n",
    "        # Extract locations\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "        \n",
    "        # Compute distance\n",
    "        distance = ((x-x_p)**2 + (y-y_p)**2 + (z-z_p)**2)**0.5\n",
    "        return distance\n",
    "\n",
    "    # Check if a state is a risky state\n",
    "    def check_risky_state(self, state):\n",
    "        # Extract locations\n",
    "        state = (state[0], state[1], state[2])\n",
    "        \n",
    "        # Check risky state\n",
    "        for location in self.risky_locations:\n",
    "            if state == location:\n",
    "                return True\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "21e60da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training module\n",
    "class Training: \n",
    "    def train(self, total_steps, alpha, gamma, algorithm1, algorithm2, policy1, policy2, flag=False):\n",
    "        \n",
    "        alg = Algorithm()\n",
    "        \n",
    "        # Initialize environment\n",
    "        environment = Environment(copy.deepcopy(alg.pickup_locations), copy.deepcopy(alg.dropoff_locations))\n",
    "        \n",
    "        # Initiate 2 agents\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(environment.states)\n",
    "        female_agent = Agent(female_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        male_agent = Agent(male_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        \n",
    "        agent_coordination = []\n",
    "        done = False\n",
    "        terminal_state_count = 0\n",
    "        \n",
    "        # Start training\n",
    "        for step in range(0, total_steps+1):\n",
    "            #if step % 100 == 0:\n",
    "                #print(\"Step [{}/{}]\".format(step, total_steps))\n",
    "        \n",
    "            # Update algorithm and policy\n",
    "            if step > 500:\n",
    "                (female_agent.algorithm, female_agent.policy) = (algorithm2, policy2)\n",
    "                (male_agent.algorithm, male_agent.policy) = (algorithm2, policy2)\n",
    "                \n",
    "            if step % 2 == 0:\n",
    "                # Female's move\n",
    "                action = female_agent.choose_action(environment, female_agent.state, male_agent.state)\n",
    "                female_agent.take_action(environment, action, male_agent.state)\n",
    "                \n",
    "                # Check if the agent move into a risky state\n",
    "                if alg.check_risky_state(female_agent.state):\n",
    "                    female_agent.risky_state_count += 1\n",
    "                \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(female_agent.state, action, True)\n",
    "            else:\n",
    "                # Male's move\n",
    "                action = male_agent.choose_action(environment, male_agent.state, female_agent.state)\n",
    "                male_agent.take_action(environment, action, female_agent.state)\n",
    "                \n",
    "                # Check if the agent move into a risky state\n",
    "                if alg.check_risky_state(male_agent.state):\n",
    "                    male_agent.risky_state_count += 1\n",
    "                    \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(male_agent.state, action, False)\n",
    "            \n",
    "            # Compute distance\n",
    "            agent_coordination.append(Algorithm().compute_distance(female_agent.state, male_agent.state))\n",
    "            \n",
    "            # Check for terminal state\n",
    "            if done:\n",
    "                terminal_state_count += 1\n",
    "                  \n",
    "                # Change pick up locations\n",
    "                if flag:\n",
    "                    if terminal_state_count == 3:\n",
    "                        alg.pickup_locations = [[(2, 3, 3), 10], [(1, 3, 1), 10]]\n",
    "                    elif terminal_state_count == 6:\n",
    "                        break\n",
    "                    \n",
    "                # Reset the environment to initial state\n",
    "                female_agent.state, male_agent.state = environment.reset(copy.deepcopy(alg.pickup_locations),\n",
    "                                                                         copy.deepcopy(alg.dropoff_locations))\n",
    "                    \n",
    "        return Evaluation(female_agent, male_agent, agent_coordination, terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6587a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation module\n",
    "class Evaluation:\n",
    "    def __init__(self, female_agent, male_agent, agent_coordination, terminal_state_count):\n",
    "        self.female_agent = female_agent\n",
    "        self.male_agent = male_agent\n",
    "        self.agent_coordination = agent_coordination\n",
    "        self.terminal_state_count = terminal_state_count\n",
    "    \n",
    "    def _plot_(self, data, z):\n",
    "        \n",
    "        # Swap row 1 with row 3\n",
    "        temp = data[1]\n",
    "        data[1] = data[3]\n",
    "        data[3] = temp\n",
    "        \n",
    "        # Define the data for the visualization\n",
    "        num_states = 9\n",
    "        num_actions = 8\n",
    "        \n",
    "        # Create a 3x3 grid of subplots\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(10, 8))\n",
    "\n",
    "        colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']\n",
    "        # Iterate over each subplot and plot a bar chart of the corresponding data\n",
    "        for i in range(num_states):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            axs[row, col].bar(range(8), data[row+1][col+1], color=colors)\n",
    "            axs[row, col].set_ylim([0, num_actions])\n",
    "            axs[row, col].set_xticks([])\n",
    "\n",
    "\n",
    "        # Set a title for the entire grid\n",
    "        fig.suptitle('Q-Values for state (*,*,{},0,1,1,0,0)'.format(z), fontsize=16)\n",
    "\n",
    "        # Add a single x-axis label that spans the entire grid\n",
    "        fig.add_subplot(111, frameon=False)\n",
    "        plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "        plt.xlabel('Actions', fontsize=14)\n",
    "\n",
    "        # Add a single y-axis label that spans the entire grid\n",
    "        fig.add_subplot(111, frameon=False)\n",
    "        plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "        plt.ylabel('Q-Value', fontsize=14, rotation=90)\n",
    "\n",
    "        # Adjust the spacing\n",
    "        fig.tight_layout(pad=2)\n",
    "\n",
    "        # Create a legend for the colors\n",
    "        legend_elements = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]\n",
    "        fig.legend(legend_elements, ['North', 'East', 'South', 'West', 'Up', 'Down', 'Pick up', 'Drop off'], loc='upper right')\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_q_table(self, agent):\n",
    "        if agent == \"male\":\n",
    "            agent = self.male_agent\n",
    "        else:\n",
    "            agent = self.female_agent\n",
    "            \n",
    "        # plot for z=1\n",
    "        data = list(np.empty((4, 4, 8), dtype=object))\n",
    "        for i in range(1, 4):\n",
    "            for j in range(1, 4):\n",
    "                    data[i][j] = agent.q_table.q_table[(i, j, 1, 0, 1, 1, 0, 0)]\n",
    "        self._plot_(data=data, z=1)\n",
    "            \n",
    "        # plot for z=2\n",
    "        data = list(np.empty((4, 4, 8), dtype=object))\n",
    "        for i in range(1, 4):\n",
    "            for j in range(1, 4):\n",
    "                    data[i][j] = agent.q_table.q_table[(i, j, 2, 0, 1, 1, 0, 0)]\n",
    "        self._plot_(data=data, z=2)    \n",
    "        \n",
    "        # plot for z=3\n",
    "        data = list(np.empty((4, 4, 8), dtype=object))\n",
    "        for i in range(1, 4):\n",
    "            for j in range(1, 4):\n",
    "                    data[i][j] = agent.q_table.q_table[(i, j, 3, 0, 1, 1, 0, 0)]\n",
    "        self._plot_(data=data, z=3)  \n",
    "        \n",
    "    def print_result(self):\n",
    "        print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "        print()\n",
    "        print('---Agent male summary---')\n",
    "        print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "        print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "        print(\"Average pick up times: \", results.male_agent.pickup_state_count/results.terminal_state_count)\n",
    "        print(\"Average drop off times: \", results.male_agent.dropoff_state_count/results.terminal_state_count)\n",
    "        print()\n",
    "        print('---Agent female summary---')\n",
    "        print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "        print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)\n",
    "        print(\"Average pick up times: \", results.female_agent.pickup_state_count/results.terminal_state_count)\n",
    "        print(\"Average drop off times: \", results.female_agent.dropoff_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d65d755b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment 1a\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PRANDOM, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e2e30aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 10\n",
      "\n",
      "---Agent male summary---\n",
      "Average reward: -221.3\n",
      "Average risky states reached: 46.5\n",
      "Average pick up times:  10.7\n",
      "Average drop off times:  10.6\n",
      "\n",
      "---Agent female summary---\n",
      "Average reward: -206.0\n",
      "Average risky states reached: 44.1\n",
      "Average pick up times:  11.1\n",
      "Average drop off times:  11.0\n"
     ]
    }
   ],
   "source": [
    "results.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "47a19f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot_q_table(\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b581dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66cfb388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cc95c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d08ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebe2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059ecbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "02b0e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 11\n",
      "\n",
      "---Agent male summary---\n",
      "Average reward: -164.0\n",
      "Average risky states reached: 39.36363636363637\n",
      "---Agent female summary---\n",
      "\n",
      "Average reward: -162.8181818181818\n",
      "Average risky states reached: 41.81818181818182\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc23af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b69094b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1b\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PGREEDY, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88a4c891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 39\n",
      "---Agent male summary---\n",
      "Average reward: 165.17948717948718\n",
      "Average risky states reached: 8.076923076923077\n",
      "---Agent female summary---\n",
      "Average reward: 177.6153846153846\n",
      "Average risky states reached: 6.128205128205129\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44555939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1c\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "adf684dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 35\n",
      "---Agent male summary---\n",
      "Average reward: 151.0857142857143\n",
      "Average risky states reached: 9.8\n",
      "---Agent female summary---\n",
      "Average reward: 166.9142857142857\n",
      "Average risky states reached: 6.942857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a75bebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af661dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 38\n",
      "---Agent male summary---\n",
      "Average reward: 168.5\n",
      "Average risky states reached: 9.289473684210526\n",
      "---Agent female summary---\n",
      "Average reward: 163.81578947368422\n",
      "Average risky states reached: 9.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be3ff91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3a\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.1, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40f89b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 27\n",
      "---Agent male summary---\n",
      "Average reward: 118.44444444444444\n",
      "Average risky states reached: 11.814814814814815\n",
      "---Agent female summary---\n",
      "Average reward: 113.51851851851852\n",
      "Average risky states reached: 12.74074074074074\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef7bcd6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3b\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.5, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "725f28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 40\n",
      "---Agent male summary---\n",
      "Average reward: 176.9\n",
      "Average risky states reached: 7.35\n",
      "---Agent female summary---\n",
      "Average reward: 169.375\n",
      "Average risky states reached: 7.475\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f4b6cdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3604c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 6\n",
      "---Agent male summary---\n",
      "Average reward: -18.833333333333332\n",
      "Average risky states reached: 23.666666666666668\n",
      "---Agent female summary---\n",
      "Average reward: -27.5\n",
      "Average risky states reached: 21.666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print('---Agent male summary---')\n",
    "print(\"Average reward:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.male_agent.risky_state_count/results.terminal_state_count)\n",
    "\n",
    "print('---Agent female summary---')\n",
    "print(\"Average reward:\", results.female_agent.reward/results.terminal_state_count)\n",
    "print(\"Average risky states reached:\", results.female_agent.risky_state_count/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm module\n",
    "class Algorithm:\n",
    "        \n",
    "    def compute_distance(self, agent_state, other_agent_state):\n",
    "        # Extract locations\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "        \n",
    "        # Compute distance\n",
    "        distance = ((x-x_p)**2 + (y-y_p)**2 + (z-z_p)**2)**0.5\n",
    "        return distance\n",
    "    \n",
    "    def run(self, environment, female_agent, male_agent):\n",
    "        agent_coordination = []\n",
    "        done = False\n",
    "        step = 1\n",
    "        \n",
    "        while not done:\n",
    "#             if step % 100 == 0:\n",
    "#                 print('Step ', step)\n",
    "                \n",
    "#             print (\"-------------------------------------------------------------\")\n",
    "#             print('World states: ', environment.states)\n",
    "            \n",
    "            # Female's move     \n",
    "#             print('agent female state before action: ', female_agent.state)\n",
    "                \n",
    "            action = female_agent.choose_action(environment, female_agent.state, male_agent.state)\n",
    "            female_agent.take_action(environment, action, male_agent.state)\n",
    "                \n",
    "            # Update world state space\n",
    "            done, female_agent.state, male_agent.state = environment.update(female_agent.state, action, True)\n",
    "                    \n",
    "            if not done:\n",
    "                # Male's move\n",
    "#                 print('agent male state before action: ', male_agent.state)    \n",
    "                action = male_agent.choose_action(environment, male_agent.state, female_agent.state)\n",
    "                male_agent.take_action(environment, action, female_agent.state)\n",
    "                    \n",
    "                \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(male_agent.state, action, False)\n",
    "\n",
    "            # Compute distance\n",
    "            agent_coordination = self.compute_distance(female_agent.state, male_agent.state)\n",
    "            \n",
    "            step += 1\n",
    "#             print (\"-------------------------------------------------------------\")\n",
    "#         print (\"-------------------------------------------------------------\")\n",
    "#         print(\"World states: \", environment.states)\n",
    "        return agent_coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56372c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training module\n",
    "class Training:\n",
    "    def __init__(self):\n",
    "        self.agent_coordinations = []\n",
    "        \n",
    "        \n",
    "    def train(self, total_episode, alpha, gamma, algorithm1, algorithm2, policy1, policy2):\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        pick_up = [[(2, 2, 1), 10], [(3, 3, 2), 10]]\n",
    "        drop_off = [[(1, 1, 2), 0], [(1, 1, 3), 0], [(3, 1, 1), 0], [(3, 2, 3), 0]]\n",
    "    \n",
    "        \n",
    "        # Initialize environment\n",
    "        environment = Environment(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "        \n",
    "        # Initiate 2 agents\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(environment.states)\n",
    "        \n",
    "        female_agent = Agent(female_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        male_agent = Agent(male_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        \n",
    "        # Start training\n",
    "        for episode in range(0, total_episode):\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                print(\"Episode [{}/{}]\".format(episode, total_episode))\n",
    "            \n",
    "            # Update algorithm and policy\n",
    "            if episode > 500:\n",
    "                (female_agent.algorithm, female_agent.policy) = (algorithm2, policy2)\n",
    "                (male_agent.algorithm, male_agent.policy) = (algorithm2, policy2)\n",
    "                \n",
    "            agent_coordination = Algorithm().run(environment, female_agent, male_agent)  \n",
    "            self.agent_coordinations.append(agent_coordination)\n",
    "            \n",
    "            # Reset the environment to initial state\n",
    "            female_agent.state, male_agent.state = environment.reset(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "            \n",
    "            \n",
    "        return female_agent.q_table, male_agent.q_table, self.agent_coordinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
