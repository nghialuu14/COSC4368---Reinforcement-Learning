{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5e766d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "70a4ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment module\n",
    "class Environment:\n",
    "    def __init__(self, pick_up, drop_off):\n",
    "        # Generate initital random location for 'Female' agent\n",
    "        x, y, z = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "        \n",
    "        # Generate initial random location for 'Male' agent\n",
    "        x_p, y_p, z_p = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "        # make sure the location is available\n",
    "        while x_p == x and y_p == y and z_p == z:\n",
    "            x_p, y_p, z_p = np.random.randint(1,4), np.random.randint(1,4), np.random.randint(1,4)\n",
    "            \n",
    "        # Define the shape of the environment (i.e., its states)\n",
    "        self.states = (x, y, z, x_p, y_p, z_p, 0, 0, 0, 0, 0, 0, 10, 10)\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        self.pick_up = pick_up\n",
    "        self.drop_off = drop_off\n",
    "    \n",
    "    def reset(self, pick_up, drop_off):\n",
    "        self.__init__(pick_up, drop_off)\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(self.states)\n",
    "        return female_agent_state, male_agent_state\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "    # Determine if a state is a terminal state\n",
    "        return state[6] == 0 and state[7] == 0 and state[8] == 5 and state[9] == 5 and state[10] == 5 \\\n",
    "            and state[11] == 5 and state[12] == 0 and state[13] == 0\n",
    "    \n",
    "    def update(self, agent_state, action, isFemale):\n",
    "        # Extract variables\n",
    "        x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f = self.states\n",
    "        \n",
    "        # Update the environment after the agent chose an action\n",
    "        xx, yy, zz, ii = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        pickup_list = [e, f]\n",
    "        dropoff_list = [a, b, c, d]\n",
    "        \n",
    "        # Decrease number of pick up at a specific location\n",
    "        if ActionSpace().actions[action] == 'pickup':\n",
    "            for j in range(len(pickup_list)):\n",
    "                if self.pick_up[j][0] == (xx, yy, zz):\n",
    "                    self.pick_up[j][1] -= 1\n",
    "                    pickup_list[j] = self.pick_up[j][1]\n",
    "                    break\n",
    "                \n",
    "        # Decrease number of drop off at a specific location\n",
    "        elif ActionSpace().actions[action] == 'dropoff':\n",
    "            for j in range(len(dropoff_list)):\n",
    "                if self.drop_off[j][0] == (xx, yy, zz):\n",
    "                    self.drop_off[j][1] += 1\n",
    "                    dropoff_list[j] = self.drop_off[j][1]\n",
    "                    break\n",
    "        \n",
    "        e, f = pickup_list[0], pickup_list[1]\n",
    "        a, b, c, d = dropoff_list[0], dropoff_list[1], dropoff_list[2], dropoff_list[3]\n",
    "        \n",
    "        # if female agent takes action\n",
    "        if isFemale:\n",
    "            x, y, z, i = xx, yy, zz, ii\n",
    "        # if male agent takes action\n",
    "        else:\n",
    "            x_p, y_p, z_p, i_p = xx, yy, zz, ii\n",
    "            \n",
    "        # update new state\n",
    "        self.states = (x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f)\n",
    "    \n",
    "        # check if new state is terminal state\n",
    "        done = self.is_terminal_state(self.states)\n",
    "        \n",
    "        # convert from world state to agents' state\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(self.states)\n",
    "        \n",
    "        return done, female_agent_state, male_agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d280e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent module\n",
    "class Agent:\n",
    "    def __init__(self, state, algorithm, policy, alpha, gamma):\n",
    "        self.q_table = ValueFunction(alpha, gamma)\n",
    "        self.algorithm = algorithm\n",
    "        self.state = state\n",
    "        self.policy = policy\n",
    "        self.reward = 0\n",
    "        \n",
    "    def choose_action(self, environment, agent_state, other_agent_state):\n",
    "        possible_actions = ActionSpace().get_possible_actions(environment, agent_state, other_agent_state)\n",
    "        action = self.policy(self.q_table, agent_state, possible_actions)\n",
    "        return action\n",
    "    \n",
    "    def take_action(self, environment, action, other_agent_state):\n",
    "    \n",
    "\n",
    "#         print(\"Agent picking action: \", ActionSpace().actions[action])\n",
    "        \n",
    "        # Determine next state and reward if the agent take the action from its current state\n",
    "        new_state, reward = Model().predict(self.state, environment.states, action)\n",
    "        \n",
    "        # Q-Learning algorithm\n",
    "        if self.algorithm == \"QLEARNING\":\n",
    "            self.q_table.QLEARNING(self.state, action, new_state, reward)\n",
    "            \n",
    "        # SARSA algorithm\n",
    "        elif self.algorithm == \"SARSA\":\n",
    "            new_action = self.choose_action(environment, new_state, other_agent_state)\n",
    "            self.q_table.SARSA(self.state, action, new_state, new_action, reward)\n",
    "        \n",
    "        # move to new state\n",
    "        self.state = new_state\n",
    "        \n",
    "        # add reward\n",
    "        self.reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "42d699e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State representation module\n",
    "class StateRepresentation:\n",
    "    def preprocess(self, state):\n",
    "        # Convert the state into a format that can be used by the agent\n",
    "        (x, y, z, x_p, y_p, z_p, i, i_p, a, b, c, d, e, f) = state\n",
    "        \n",
    "        # Convert into female agent state\n",
    "        if i == 0:\n",
    "            s, t, u, v = state[12] >= 1, state[13] >= 1, False, False\n",
    "        else:\n",
    "            s, t, u, v = state[8] < 5, state[9] < 5, state[10] < 5, state[11] < 5\n",
    "         \n",
    "        female_agent_state = (x, y, z, i, s, t, u, v)\n",
    "        \n",
    "        # Convert into male agent state\n",
    "        if i_p == 0:\n",
    "            s, t, u, v = state[12] >= 1, state[13] >= 1, False, False\n",
    "        else:\n",
    "            s, t, u, v = state[8] < 5, state[9] < 5, state[10] < 5, state[11] < 5\n",
    "         \n",
    "        male_agent_state = (x_p, y_p, z_p, i_p, s, t, u, v)\n",
    "            \n",
    "        return female_agent_state, male_agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "611ec56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action space module\n",
    "class ActionSpace:\n",
    "    def __init__(self):\n",
    "        # Initialize the action space with the possible actions\n",
    "        self.actions = ['north', 'east', 'south', 'west', 'up', 'down', 'pickup', 'dropoff']\n",
    "    \n",
    "    # Check if action 'pick up' is applicable given an agent state\n",
    "    def CheckPickUp(self, pick_up, agent_state):\n",
    "        # Extract variables\n",
    "        x, y, z, i = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        # check if pick up is applicable\n",
    "        for locations in pick_up:\n",
    "            if locations[0] == (x, y, z) and locations[1] > 0 and i == 0:\n",
    "                return True\n",
    "    \n",
    "    # Check if action 'drop off' is applicable given an agent state\n",
    "    def CheckDropOff(self, drop_off, agent_state):\n",
    "        # Extract variables\n",
    "        x, y, z, i = agent_state[0], agent_state[1], agent_state[2], agent_state[3]\n",
    "        \n",
    "        # check if drop off is applicable\n",
    "        for locations in drop_off:\n",
    "            if locations[0] == (x, y, z) and locations[1] < 5 and i == 1:\n",
    "                return True\n",
    "    \n",
    "    # Return a list of possible actions given the agent state and other agent state (this also handle collision between agents)\n",
    "    def get_possible_actions(self, environment, agent_state, other_agent_state):\n",
    "        \n",
    "        # Initiate all actions '0'\n",
    "        possible_actions = np.zeros(8)\n",
    "        \n",
    "        # Extract agent's location and other agent's location\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "    \n",
    "        # Check if agent can move north\n",
    "        if agent_state[1] < 3 and (x, y+1, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[0] = 1\n",
    "        # Check if agent can move east\n",
    "        if agent_state[0] < 3 and (x+1, y, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[1] = 1\n",
    "        # Check if agent can move south\n",
    "        if agent_state[1] > 1 and (x, y-1, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[2] = 1    \n",
    "        # Check if agent can move west\n",
    "        if agent_state[0] > 1 and (x-1, y, z) != (x_p, y_p, z_p):\n",
    "            possible_actions[3] = 1\n",
    "        # Check if agent can move up\n",
    "        if agent_state[2] < 3 and (x, y, z+1) != (x_p, y_p, z_p):\n",
    "            possible_actions[4] = 1\n",
    "        # Check if agent can move down\n",
    "        if agent_state[2] > 1 and (x, y, z-1) != (x_p, y_p, z_p):\n",
    "            possible_actions[5] = 1\n",
    "        # Check if agent can pick up\n",
    "        if self.CheckPickUp(environment.pick_up, agent_state):\n",
    "            possible_actions[6] = 1\n",
    "        # Check if agent can drop off\n",
    "        if self.CheckDropOff(environment.drop_off, agent_state):\n",
    "            possible_actions[7] = 1\n",
    "        \n",
    "        return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a6f792a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function module\n",
    "class RewardFunction:\n",
    "    def __init__(self):\n",
    "        # Define rewards for each state\n",
    "        self.rewards = np.full((4, 4, 4), -1)\n",
    "        self.rewards[3, 1, 1] = -2 # Risky state\n",
    "        self.rewards[2, 2, 2] = -2 # Risky state\n",
    "\n",
    "    # Calculate the reward based on an action that takes agent to the given state\n",
    "    def calculate(self, state, action):\n",
    "        \n",
    "        # Agent's current location\n",
    "        x, y, z = state[0], state[1], state[2]\n",
    "\n",
    "        # Reward\n",
    "        reward = -1\n",
    "        \n",
    "        # If action was 'pick up' or 'drop off'\n",
    "        if action == 6 or action == 7:\n",
    "            reward = 14\n",
    "        # If action was moving agent\n",
    "        else:\n",
    "            reward = self.rewards[x, y, z]\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "da6e4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy module\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.Actions = ActionSpace()\n",
    "    \n",
    "    def PRANDOM(self, q_table, agent_state, possible_actions):   \n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # choosing action randomly\n",
    "        action = np.random.randint(6)\n",
    "        while possible_actions[action] == 0:\n",
    "            action = np.random.randint(6)\n",
    "            \n",
    "        return action   \n",
    "    \n",
    "    def PEXPLOIT(self, q_table, agent_state, possible_actions):        \n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # 85% choosing the optimal action (action with highest q-value)\n",
    "        if np.random.random() < 0.85:\n",
    "            maxq = -100\n",
    "\n",
    "            for actions in range(6):\n",
    "                if possible_actions[actions] == 1:\n",
    "                    if q_table.get_value(agent_state, actions) > maxq:\n",
    "                        maxq = q_table.get_value(agent_state, actions)\n",
    "                        action = actions\n",
    "                    elif q_table.get_value(agent_state, actions) == maxq:\n",
    "                        action = action if np.random.randint(2) == 0 else actions\n",
    "        else:\n",
    "            # 15% choosing action randomly\n",
    "            action = np.random.randint(6)\n",
    "            while possible_actions[action] == 0:\n",
    "                action = np.random.randint(6)\n",
    "            \n",
    "        return action   \n",
    "    \n",
    "    \n",
    "    def PGREEDY(self, q_table, agent_state, possible_actions):\n",
    "        \n",
    "        # Check if either pick up or drop off is applicable\n",
    "        if possible_actions[6] == 1:\n",
    "            return 6\n",
    "        elif possible_actions[7] == 1:\n",
    "            return 7\n",
    "        \n",
    "        # choosing action with the highest q-value\n",
    "        maxq = -100\n",
    "        \n",
    "        for actions in range(6):\n",
    "            if possible_actions[actions] == 1:\n",
    "                if q_table.get_value(agent_state, actions) > maxq:\n",
    "                    maxq = q_table.get_value(agent_state, actions)\n",
    "                    action = actions\n",
    "                elif q_table.get_value(agent_state, actions) == maxq:\n",
    "                    action = action if np.random.randint(2) == 0 else actions\n",
    "                \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f6d2417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function module\n",
    "class ValueFunction:\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {}\n",
    "        \n",
    "        # Initiate q-table (dictionary with key is state, and value is list of 6 actions)\n",
    "        # q_table[state][action] gives the actual q-value of a state-action pair\n",
    "        for x in range(1, 4):\n",
    "            for y in range(1, 4):\n",
    "                for z in range(1, 4):\n",
    "                    for i in range(0, 2):\n",
    "                        for s in range(0, 2):\n",
    "                            for t in range(0, 2):\n",
    "                                for u in range(0, 2):\n",
    "                                    for v in range(0, 2):\n",
    "                                        self.q_table[(x, y, z, i, s, t, u, v)] = []\n",
    "                                        for action in range(8):\n",
    "                                            self.q_table[(x, y, z, i, s, t, u, v)].append(0)\n",
    "                                            \n",
    "    # Update Q-Table\n",
    "    def QLEARNING(self, state, action, new_state, reward):\n",
    "        self.q_table[state][action] = self.q_table[state][action] \\\n",
    "                                            + self.alpha*(reward + self.gamma*np.max(self.q_table[new_state]) \\\n",
    "                                            - self.q_table[state][action])\n",
    "        \n",
    "    def SARSA(self, state, action, new_state, new_action, reward):\n",
    "        self.q_table[state][action] = self.q_table[state][action] \\\n",
    "                                            + self.alpha*(reward + self.gamma*self.q_table[new_state][new_action] \\\n",
    "                                            - self.q_table[state][action])\n",
    "        \n",
    "    # Get a specific Q-value in the Q-Table\n",
    "    def get_value(self, state, action):\n",
    "        return self.q_table[state][action]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b3f2a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model module\n",
    "class Model:\n",
    "    \n",
    "    # Return next state and its reward given the agent current state, world state, and action performed by the agent\n",
    "    def predict(self, agent_state, states, action):\n",
    "        \n",
    "        # Extract variables\n",
    "        x, y, z, i, s, t, u, v = agent_state[0], agent_state[1], agent_state[2], agent_state[3], agent_state[4], agent_state[5], agent_state[6], agent_state[7]\n",
    "        \n",
    "        # move north\n",
    "        if ActionSpace().actions[action] == 'north' and y < 3:\n",
    "            y = y + 1\n",
    "        # move east\n",
    "        elif ActionSpace().actions[action] == 'east' and x < 3:\n",
    "            x = x + 1\n",
    "        # move south\n",
    "        elif ActionSpace().actions[action] == 'south' and y > 1:\n",
    "            y = y - 1\n",
    "        # move west\n",
    "        elif ActionSpace().actions[action] == 'west' and x > 1:\n",
    "            x = x - 1\n",
    "        # move up\n",
    "        elif ActionSpace().actions[action] == 'up' and z < 3:\n",
    "            z = z + 1\n",
    "        # move down\n",
    "        elif ActionSpace().actions[action] == 'down' and z > 1:\n",
    "            z = z - 1\n",
    "        # pick up\n",
    "        elif ActionSpace().actions[action] == 'pickup' and i == 0:\n",
    "            i = 1\n",
    "        # drop off\n",
    "        elif ActionSpace().actions[action] == 'dropoff' and i == 1:\n",
    "            i = 0\n",
    "            \n",
    "        if i == 0:\n",
    "            s = 1 if states[12] >= 1 else 0\n",
    "            t = 1 if states[13] >= 1 else 0\n",
    "            u = 0\n",
    "            v = 0\n",
    "            \n",
    "        else:\n",
    "            s = 1 if states[8] < 5 else 0\n",
    "            t = 1 if states[9] < 5 else 0\n",
    "            u = 1 if states[10] < 5 else 0\n",
    "            v = 1 if states[11] < 5 else 0\n",
    "        \n",
    "        # new state\n",
    "        next_state = (x, y, z, i, s, t, u, v)\n",
    "        \n",
    "        # reward\n",
    "        reward = RewardFunction().calculate(next_state, action)\n",
    "        \n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "dbe3cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm module\n",
    "class Algorithm:     \n",
    "    def compute_distance(self, agent_state, other_agent_state):\n",
    "        # Extract locations\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "        \n",
    "        # Compute distance\n",
    "        distance = ((x-x_p)**2 + (y-y_p)**2 + (z-z_p)**2)**0.5\n",
    "        return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "21e60da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training module\n",
    "class Training: \n",
    "    def train(self, total_steps, alpha, gamma, algorithm1, algorithm2, policy1, policy2, flag=False):\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        pick_up = [[(2, 2, 1), 10], [(3, 3, 2), 10]]\n",
    "        drop_off = [[(1, 1, 2), 0], [(1, 1, 3), 0], [(3, 1, 1), 0], [(3, 2, 3), 0]]\n",
    "    \n",
    "        # Initialize environment\n",
    "        environment = Environment(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "        \n",
    "        # Initiate 2 agents\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(environment.states)\n",
    "        female_agent = Agent(female_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        male_agent = Agent(male_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        \n",
    "        agent_coordination = []\n",
    "        done = False\n",
    "        terminal_state_count = 0\n",
    "        \n",
    "        # Start training\n",
    "        for step in range(0, total_steps+1):\n",
    "            if step % 100 == 0:\n",
    "                print(\"Step [{}/{}]\".format(step, total_steps))\n",
    "        \n",
    "            # Update algorithm and policy\n",
    "            if step > 500:\n",
    "                (female_agent.algorithm, female_agent.policy) = (algorithm2, policy2)\n",
    "                (male_agent.algorithm, male_agent.policy) = (algorithm2, policy2)\n",
    "                \n",
    "            if step % 2 == 0:\n",
    "                # Female's move\n",
    "                action = female_agent.choose_action(environment, female_agent.state, male_agent.state)\n",
    "                female_agent.take_action(environment, action, male_agent.state)\n",
    "                \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(female_agent.state, action, True)\n",
    "            else:\n",
    "                # Male's move\n",
    "                action = male_agent.choose_action(environment, male_agent.state, female_agent.state)\n",
    "                male_agent.take_action(environment, action, female_agent.state)\n",
    "                \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(male_agent.state, action, False)\n",
    "            \n",
    "            # Compute distance\n",
    "            agent_coordination.append(Algorithm().compute_distance(female_agent.state, male_agent.state))\n",
    "            \n",
    "            # Check for terminal state\n",
    "            if done:\n",
    "                terminal_state_count += 1\n",
    "                  \n",
    "                # Change pick up locations\n",
    "                if flag and terminal_state_count == 3:\n",
    "                    pick_up = [[(2, 3, 3), 10], [(1, 3, 1), 10]]\n",
    "                    \n",
    "                # Reset the environment to initial state\n",
    "                female_agent.state, male_agent.state = environment.reset(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "                    \n",
    "        return Evaluation(female_agent, male_agent, agent_coordination, terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "6587a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation module\n",
    "class Evaluation:\n",
    "    def __init__(self, female_agent, male_agent, agent_coordination, terminal_state_count):\n",
    "        self.female_agent = female_agent\n",
    "        self.male_agent = male_agent\n",
    "        self.agent_coordination = agent_coordination\n",
    "        self.terminal_state_count = terminal_state_count\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Evaluate the agent's performance by measuring its average reward, success rate, and other relevant metrics\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb911e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "d65d755b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1a\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PRANDOM, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "02b0e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 10\n",
      "Average reward for agent male: -206.5\n",
      "Average reward for agent female: -221.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print(\"Average reward for agent male:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average reward for agent female:\", results.female_agent.reward/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "7b69094b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1b\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PGREEDY, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "88a4c891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 42\n",
      "Average reward for agent male: 179.8095238095238\n",
      "Average reward for agent female: 180.97619047619048\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print(\"Average reward for agent male:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average reward for agent female:\", results.female_agent.reward/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "44555939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1c\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"QLEARNING\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "adf684dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 34\n",
      "Average reward for agent male: 154.61764705882354\n",
      "Average reward for agent female: 154.7058823529412\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print(\"Average reward for agent male:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average reward for agent female:\", results.female_agent.reward/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.3, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "be3ff91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3a\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.1, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3b\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.5, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "f4b6cdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/10000]\n",
      "Step [100/10000]\n",
      "Step [200/10000]\n",
      "Step [300/10000]\n",
      "Step [400/10000]\n",
      "Step [500/10000]\n",
      "Step [600/10000]\n",
      "Step [700/10000]\n",
      "Step [800/10000]\n",
      "Step [900/10000]\n",
      "Step [1000/10000]\n",
      "Step [1100/10000]\n",
      "Step [1200/10000]\n",
      "Step [1300/10000]\n",
      "Step [1400/10000]\n",
      "Step [1500/10000]\n",
      "Step [1600/10000]\n",
      "Step [1700/10000]\n",
      "Step [1800/10000]\n",
      "Step [1900/10000]\n",
      "Step [2000/10000]\n",
      "Step [2100/10000]\n",
      "Step [2200/10000]\n",
      "Step [2300/10000]\n",
      "Step [2400/10000]\n",
      "Step [2500/10000]\n",
      "Step [2600/10000]\n",
      "Step [2700/10000]\n",
      "Step [2800/10000]\n",
      "Step [2900/10000]\n",
      "Step [3000/10000]\n",
      "Step [3100/10000]\n",
      "Step [3200/10000]\n",
      "Step [3300/10000]\n",
      "Step [3400/10000]\n",
      "Step [3500/10000]\n",
      "Step [3600/10000]\n",
      "Step [3700/10000]\n",
      "Step [3800/10000]\n",
      "Step [3900/10000]\n",
      "Step [4000/10000]\n",
      "Step [4100/10000]\n",
      "Step [4200/10000]\n",
      "Step [4300/10000]\n",
      "Step [4400/10000]\n",
      "Step [4500/10000]\n",
      "Step [4600/10000]\n",
      "Step [4700/10000]\n",
      "Step [4800/10000]\n",
      "Step [4900/10000]\n",
      "Step [5000/10000]\n",
      "Step [5100/10000]\n",
      "Step [5200/10000]\n",
      "Step [5300/10000]\n",
      "Step [5400/10000]\n",
      "Step [5500/10000]\n",
      "Step [5600/10000]\n",
      "Step [5700/10000]\n",
      "Step [5800/10000]\n",
      "Step [5900/10000]\n",
      "Step [6000/10000]\n",
      "Step [6100/10000]\n",
      "Step [6200/10000]\n",
      "Step [6300/10000]\n",
      "Step [6400/10000]\n",
      "Step [6500/10000]\n",
      "Step [6600/10000]\n",
      "Step [6700/10000]\n",
      "Step [6800/10000]\n",
      "Step [6900/10000]\n",
      "Step [7000/10000]\n",
      "Step [7100/10000]\n",
      "Step [7200/10000]\n",
      "Step [7300/10000]\n",
      "Step [7400/10000]\n",
      "Step [7500/10000]\n",
      "Step [7600/10000]\n",
      "Step [7700/10000]\n",
      "Step [7800/10000]\n",
      "Step [7900/10000]\n",
      "Step [8000/10000]\n",
      "Step [8100/10000]\n",
      "Step [8200/10000]\n",
      "Step [8300/10000]\n",
      "Step [8400/10000]\n",
      "Step [8500/10000]\n",
      "Step [8600/10000]\n",
      "Step [8700/10000]\n",
      "Step [8800/10000]\n",
      "Step [8900/10000]\n",
      "Step [9000/10000]\n",
      "Step [9100/10000]\n",
      "Step [9200/10000]\n",
      "Step [9300/10000]\n",
      "Step [9400/10000]\n",
      "Step [9500/10000]\n",
      "Step [9600/10000]\n",
      "Step [9700/10000]\n",
      "Step [9800/10000]\n",
      "Step [9900/10000]\n",
      "Step [10000/10000]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4\n",
    "TrainedModel = Training()\n",
    "results = TrainedModel.train(10000, 0.5, 0.5, \"QLEARNING\", \"SARSA\", Policy().PRANDOM, Policy().PEXPLOIT, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "3604c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terminal states reached: 31\n",
      "Average reward for agent male: 132.2258064516129\n",
      "Average reward for agent female: 147.09677419354838\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terminal states reached:\", results.terminal_state_count)\n",
    "print(\"Average reward for agent male:\", results.male_agent.reward/results.terminal_state_count)\n",
    "print(\"Average reward for agent female:\", results.female_agent.reward/results.terminal_state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "6202190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.625\n"
     ]
    }
   ],
   "source": [
    "state = (1,2,1,1,1,1,0,0)\n",
    "action = 0 #east\n",
    "print(results.male_agent.q_table.get_value(state,action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm module\n",
    "class Algorithm:\n",
    "        \n",
    "    def compute_distance(self, agent_state, other_agent_state):\n",
    "        # Extract locations\n",
    "        x, y, z = agent_state[0], agent_state[1], agent_state[2]\n",
    "        x_p, y_p, z_p = other_agent_state[0], other_agent_state[1], other_agent_state[2]\n",
    "        \n",
    "        # Compute distance\n",
    "        distance = ((x-x_p)**2 + (y-y_p)**2 + (z-z_p)**2)**0.5\n",
    "        return distance\n",
    "    \n",
    "    def run(self, environment, female_agent, male_agent):\n",
    "        agent_coordination = []\n",
    "        done = False\n",
    "        step = 1\n",
    "        \n",
    "        while not done:\n",
    "#             if step % 100 == 0:\n",
    "#                 print('Step ', step)\n",
    "                \n",
    "#             print (\"-------------------------------------------------------------\")\n",
    "#             print('World states: ', environment.states)\n",
    "            \n",
    "            # Female's move     \n",
    "#             print('agent female state before action: ', female_agent.state)\n",
    "                \n",
    "            action = female_agent.choose_action(environment, female_agent.state, male_agent.state)\n",
    "            female_agent.take_action(environment, action, male_agent.state)\n",
    "                \n",
    "            # Update world state space\n",
    "            done, female_agent.state, male_agent.state = environment.update(female_agent.state, action, True)\n",
    "                    \n",
    "            if not done:\n",
    "                # Male's move\n",
    "#                 print('agent male state before action: ', male_agent.state)    \n",
    "                action = male_agent.choose_action(environment, male_agent.state, female_agent.state)\n",
    "                male_agent.take_action(environment, action, female_agent.state)\n",
    "                    \n",
    "                \n",
    "                # Update world state space\n",
    "                done, female_agent.state, male_agent.state = environment.update(male_agent.state, action, False)\n",
    "\n",
    "            # Compute distance\n",
    "            agent_coordination = self.compute_distance(female_agent.state, male_agent.state)\n",
    "            \n",
    "            step += 1\n",
    "#             print (\"-------------------------------------------------------------\")\n",
    "#         print (\"-------------------------------------------------------------\")\n",
    "#         print(\"World states: \", environment.states)\n",
    "        return agent_coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56372c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training module\n",
    "class Training:\n",
    "    def __init__(self):\n",
    "        self.agent_coordinations = []\n",
    "        \n",
    "        \n",
    "    def train(self, total_episode, alpha, gamma, algorithm1, algorithm2, policy1, policy2):\n",
    "        \n",
    "        # Pick up and drop off locations\n",
    "        pick_up = [[(2, 2, 1), 10], [(3, 3, 2), 10]]\n",
    "        drop_off = [[(1, 1, 2), 0], [(1, 1, 3), 0], [(3, 1, 1), 0], [(3, 2, 3), 0]]\n",
    "    \n",
    "        \n",
    "        # Initialize environment\n",
    "        environment = Environment(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "        \n",
    "        # Initiate 2 agents\n",
    "        female_agent_state, male_agent_state = StateRepresentation().preprocess(environment.states)\n",
    "        \n",
    "        female_agent = Agent(female_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        male_agent = Agent(male_agent_state, algorithm1, policy1, alpha, gamma)\n",
    "        \n",
    "        # Start training\n",
    "        for episode in range(0, total_episode):\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                print(\"Episode [{}/{}]\".format(episode, total_episode))\n",
    "            \n",
    "            # Update algorithm and policy\n",
    "            if episode > 500:\n",
    "                (female_agent.algorithm, female_agent.policy) = (algorithm2, policy2)\n",
    "                (male_agent.algorithm, male_agent.policy) = (algorithm2, policy2)\n",
    "                \n",
    "            agent_coordination = Algorithm().run(environment, female_agent, male_agent)  \n",
    "            self.agent_coordinations.append(agent_coordination)\n",
    "            \n",
    "            # Reset the environment to initial state\n",
    "            female_agent.state, male_agent.state = environment.reset(copy.deepcopy(pick_up), copy.deepcopy(drop_off))\n",
    "            \n",
    "            \n",
    "        return female_agent.q_table, male_agent.q_table, self.agent_coordinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
